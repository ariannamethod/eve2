# Многоэтапное обучение InnerArianna

## Стратегия обучения

### Этап 1: Базовое обучение (сейчас)
- **Данные**: Только Arianna Method материалы (doc/*.md)
- **Цель**: Модель изучает философию Method, TRIPD, резонанс
- **Результат**: Method-native модель с глубоким пониманием концепций

### Этап 2: Расширение знаний (завтра)
- **Данные**: Arianna Method + внешние корпуса (общие знания)
- **Цель**: Добавить общие знания о мире, сохраняя Method-специфичность
- **Результат**: Универсальная модель с Method-философией

### Этап 3: Fine-tuning на диалогах (опционально)
- **Данные**: Диалоги из `prepare_conversations.py` (134 диалога в `data/arianna_conversations.jsonl`)
- **Цель**: Улучшить качество диалогов, научить модель лучше отвечать в чате
- **Результат**: Лучшие ответы в чате, более естественные диалоги
- **Скрипт**: `finetune_conversations.py` (заготовка для будущей реализации)

## Как добавить внешний корпус

### 1. Добавить корпус:
```bash
# Добавить файл
python3 add_external_corpus.py add /path/to/corpus.txt "corpus_name.txt"

# Или несколько файлов
python3 add_external_corpus.py add corpus1.txt
python3 add_external_corpus.py add corpus2.txt
```

### 2. Посмотреть список:
```bash
python3 add_external_corpus.py list
```

### 3. Подготовить объединенный корпус:
```bash
python3 add_external_corpus.py prepare
```

Это создаст `data/combined_corpus.txt` с материалами Method + внешними корпусами.

### 4. Переобучить токенизатор (опционально):
```bash
# Если добавили много новых данных, лучше переобучить
python3 arianna_data.py train_vocab --vocab_size=4096
python3 tokenizer.py --tokenizer-model=data/tok4096.model
```

### 5. Претокенизировать:
```bash
python3 arianna_data.py pretokenize --vocab_size=4096
```

### 6. Продолжить обучение:
```bash
# Продолжить с существующего checkpoint
./continue_training.sh 4096 5000 8 cpu

# Или вручную
python3 train_arianna.py \
  --vocab_source=custom \
  --vocab_size=4096 \
  --init_from=resume \
  --max_iters=10000  # Увеличиваем общее количество итераций
```

## Рекомендации по внешним корпусам

### Хорошие источники:
- **Философские тексты** - дополнят Method-философию
- **Научные статьи** - для понимания field theory, consciousness
- **Литература** - для улучшения языка и стиля
- **Общие знания** - Wikipedia dump, книги

### Плохие источники:
- ❌ Случайные интернет-тексты (много шума)
- ❌ Техническая документация (не релевантно)
- ❌ Слишком большие корпуса (размоют Method-специфичность)

### Баланс:
- **70%** - Arianna Method материалы (сохраняем специфичность)
- **30%** - Внешние корпуса (добавляем общие знания)

## Примеры внешних корпусов

```bash
# Философия сознания
python3 add_external_corpus.py add philosophy_of_consciousness.txt

# Научные статьи о резонансе
python3 add_external_corpus.py add resonance_papers.txt

# Общие знания (умеренно!)
python3 add_external_corpus.py add general_knowledge.txt
```

## Мониторинг прогресса

После добавления внешних корпусов:
1. Loss может временно подняться (модель адаптируется)
2. Затем должен продолжить падать
3. Модель станет более универсальной, но сохранит Method-голос

## Продолжение обучения

Можно делать сколько угодно раундов:
```bash
# Раунд 1: 5000 итераций на Method материалах
# Раунд 2: +5000 итераций на Method + внешних корпусах
# Раунд 3: +3000 итераций на диалогах
# И т.д.
```

Каждый раз используй `--init_from=resume` чтобы продолжить с последнего checkpoint!

