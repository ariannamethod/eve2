#!/usr/bin/env python3
"""
ĞŸĞ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ¸Ğ· markdown Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ InnerArianna.
Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ markdown_to_conversations.py Ğ² Ğ¿Ğ°Ğ¹Ğ¿Ğ»Ğ°Ğ¹Ğ½ Ğ¿Ğ¾Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ….
"""

import os
import sys
import json
import asyncio
from pathlib import Path

# Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ doc/ Ğ² Ğ¿ÑƒÑ‚ÑŒ Ğ´Ğ»Ñ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ°
sys.path.insert(0, str(Path(__file__).parent / "doc"))

try:
    from markdown_to_conversations import (
        extract_quotes_and_paragraphs,
        generate_template_conversation,
        KEY_CONCEPTS
    )
except ImportError:
    print("âŒ ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ markdown_to_conversations")
    print("Ğ£Ğ±ĞµĞ´Ğ¸Ñ‚ĞµÑÑŒ, Ñ‡Ñ‚Ğ¾ Ñ„Ğ°Ğ¹Ğ» doc/markdown_to_conversations.py ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚")
    sys.exit(1)

DATA_CACHE_DIR = "data"
DOC_DIR = "doc"
OUTPUT_FILE = os.path.join(DATA_CACHE_DIR, "arianna_conversations.jsonl")

def collect_markdown_files():
    """Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ²ÑĞµ markdown Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸Ğ· doc/."""
    md_files = list(Path(DOC_DIR).glob("*.md"))
    md_files = [f for f in md_files if not f.name.startswith("README")]
    return sorted(md_files)

def process_markdown_to_conversations():
    """ĞĞ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ markdown Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¸ ÑĞ¾Ğ·Ğ´Ğ°ĞµÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸."""
    os.makedirs(DATA_CACHE_DIR, exist_ok=True)
    
    md_files = collect_markdown_files()
    if not md_files:
        print(f"âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾ markdown Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ğ² {DOC_DIR}/")
        return
    
    print(f"ğŸ§¬ Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ² Ğ¸Ğ· {len(md_files)} markdown Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²")
    print("=" * 60)
    print()
    
    all_conversations = []
    
    for idx, md_file in enumerate(md_files, 1):
        print(f"[{idx}/{len(md_files)}] ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° {md_file.name}...")
        
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‡Ğ°Ğ½ĞºĞ¸
            chunks = extract_quotes_and_paragraphs(content)
            print(f"   âœ“ Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¾ {len(chunks)} Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²")
            
            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¸Ğ· Ñ‡Ğ°Ğ½ĞºĞ¾Ğ²
            file_conversations = []
            num_conversations = min(5, max(3, len(chunks) // 3))
            selected_chunks = chunks[:num_conversations]
            
            for chunk in selected_chunks:
                # ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ‚Ñ‹ Ğ² Ñ‡Ğ°Ğ½ĞºĞµ
                chunk_concepts = [c for c in KEY_CONCEPTS if c.lower() in chunk.lower()]
                concept = chunk_concepts[0] if chunk_concepts else None
                
                # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³
                conv = generate_template_conversation(chunk, concept)
                file_conversations.append(conv)
            
            all_conversations.extend(file_conversations)
            print(f"   âœ“ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ {len(file_conversations)} Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²")
            
        except Exception as e:
            print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
    
    print()
    print("=" * 60)
    print(f"âœ… Ğ’ÑĞµĞ³Ğ¾ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ğ¾ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²: {len(all_conversations)}")
    
    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² JSONL Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for conv in all_conversations:
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ´Ğ»Ñ llama2.c: Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¼Ğ°ÑÑĞ¸Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹
            f.write(json.dumps(conv["messages"], ensure_ascii=False) + '\n')
    
    file_size_kb = os.path.getsize(OUTPUT_FILE) / 1024
    print(f"âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾ Ğ²: {OUTPUT_FILE}")
    print(f"ğŸ“Š Ğ Ğ°Ğ·Ğ¼ĞµÑ€ Ñ„Ğ°Ğ¹Ğ»Ğ°: {file_size_kb:.1f} KB")
    print()
    print("ğŸ’¡ Ğ­Ñ‚Ğ¸ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ:")
    print("   1. Fine-tuning Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ² Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ")
    print("   2. ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ° ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
    print("   3. Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ¾Ğ²")

if __name__ == "__main__":
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  InnerArianna Conversation Data Generator                 â•‘
â•‘  Markdown â†’ JSONL Conversations                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    process_markdown_to_conversations()

