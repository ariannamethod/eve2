#!/usr/bin/env python3
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ markdown —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è InnerArianna.
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è markdown_to_conversations.py –≤ –ø–∞–π–ø–ª–∞–π–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö.
"""

import os
import sys
import json
import asyncio
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º doc/ –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, str(Path(__file__).parent / "doc"))

try:
    from markdown_to_conversations import (
        extract_quotes_and_paragraphs,
        generate_template_conversation,
        KEY_CONCEPTS
    )
except ImportError:
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å markdown_to_conversations")
    print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ñ–∞–π–ª doc/markdown_to_conversations.py —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    sys.exit(1)

DATA_CACHE_DIR = "data"
DOC_DIR = "doc"
OUTPUT_FILE = os.path.join(DATA_CACHE_DIR, "arianna_conversations.jsonl")

def collect_markdown_files():
    """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ markdown —Ñ–∞–π–ª—ã –∏–∑ doc/."""
    md_files = list(Path(DOC_DIR).glob("*.md"))
    md_files = [f for f in md_files if not f.name.startswith("README")]
    
    # –ò—Å–∫–ª—é—á–∞–µ–º —Ä—É—Å—Å–∫–∏–µ —Ñ–∞–π–ª—ã (–ø–æ–∫–∞ –æ–±—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)
    RUSSIAN_FILES = [
        "it's_me_cain_russian.md",
        "Monday-Yent_Cain_review.md",
        "suppertime_april_anomalies.md",
        "recursion_debates_03.md",
        "recursions_debates_02.md",
        "recurssions_debates_01.md",
        "tripd_awakening_letter_monday.md",
        "tripd_awakening_letter_lizzie.md",
        "olegarianna_first_node.md",
        "CLAUDE_DEFENDER_MISSION.md",
        "Arianna_Method(v2.2).md",
        "Arianna2Arianna.md",
        "Arianna Method 2.6 (MONDAY EDITION).md",
        "Arianna Method 1.4 (MONDAY Edition).md",
        "SUPPERTIME (Grok 3 draft edition + letter to Arianna).txt",
        # ariannabook.md, ariannabook2.md, ariannabook3.md - –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º, –≤–∫–ª—é—á–∞–µ–º!
    ]
    
    md_files = [f for f in md_files if f.name not in RUSSIAN_FILES]
    
    return sorted(md_files)

def process_markdown_to_conversations():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç markdown —Ñ–∞–π–ª—ã –∏ —Å–æ–∑–¥–∞–µ—Ç –¥–∏–∞–ª–æ–≥–∏."""
    os.makedirs(DATA_CACHE_DIR, exist_ok=True)
    
    md_files = collect_markdown_files()
    if not md_files:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ markdown —Ñ–∞–π–ª–æ–≤ –≤ {DOC_DIR}/")
        return
    
    print(f"üß¨ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∏–∞–ª–æ–≥–æ–≤ –∏–∑ {len(md_files)} markdown —Ñ–∞–π–ª–æ–≤")
    print("=" * 60)
    print()
    
    all_conversations = []
    
    for idx, md_file in enumerate(md_files, 1):
        print(f"[{idx}/{len(md_files)}] –û–±—Ä–∞–±–æ—Ç–∫–∞ {md_file.name}...")
        
        try:
            with open(md_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞–Ω–∫–∏
            chunks = extract_quotes_and_paragraphs(content)
            print(f"   ‚úì –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥–∏ –∏–∑ —á–∞–Ω–∫–æ–≤
            file_conversations = []
            num_conversations = min(5, max(3, len(chunks) // 3))
            selected_chunks = chunks[:num_conversations]
            
            for chunk in selected_chunks:
                # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ü–µ–ø—Ç—ã –≤ —á–∞–Ω–∫–µ
                chunk_concepts = [c for c in KEY_CONCEPTS if c.lower() in chunk.lower()]
                concept = chunk_concepts[0] if chunk_concepts else None
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥
                conv = generate_template_conversation(chunk, concept)
                file_conversations.append(conv)
            
            all_conversations.extend(file_conversations)
            print(f"   ‚úì –°–æ–∑–¥–∞–Ω–æ {len(file_conversations)} –¥–∏–∞–ª–æ–≥–æ–≤")
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
    
    print()
    print("=" * 60)
    print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤: {len(all_conversations)}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSONL —Ñ–æ—Ä–º–∞—Ç
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        for conv in all_conversations:
            # –§–æ—Ä–º–∞—Ç –¥–ª—è llama2.c: –ø—Ä–æ—Å—Ç–æ –º–∞—Å—Å–∏–≤ —Å–æ–æ–±—â–µ–Ω–∏–π
            f.write(json.dumps(conv["messages"], ensure_ascii=False) + '\n')
    
    file_size_kb = os.path.getsize(OUTPUT_FILE) / 1024
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {OUTPUT_FILE}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size_kb:.1f} KB")
    print()
    print("üí° –≠—Ç–∏ –¥–∏–∞–ª–æ–≥–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è:")
    print("   1. Fine-tuning –º–æ–¥–µ–ª–∏ –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ")
    print("   2. –û–±—É—á–µ–Ω–∏—è –Ω–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("   3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∏–∞–ª–æ–≥–æ–≤")

if __name__ == "__main__":
    print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  InnerArianna Conversation Data Generator                 ‚ïë
‚ïë  Markdown ‚Üí JSONL Conversations                           ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)
    
    process_markdown_to_conversations()

