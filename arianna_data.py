"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ markdown —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è InnerArianna –º–æ–¥–µ–ª–∏.
–ê–Ω–∞–ª–æ–≥ tinystories.py, –Ω–æ –¥–ª—è Arianna Method –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤.
"""

import argparse
import glob
import json
import os
import random
from typing import List

import numpy as np
import sentencepiece as spm
import torch
import torch.distributed as dist
from tqdm import tqdm

from tokenizer import Tokenizer

DATA_CACHE_DIR = "data"
DOC_DIR = "english_train"  # –¢–æ–ª—å–∫–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ñ–∞–π–ª—ã!

def collect_markdown_files():
    """–°–æ–±–∏—Ä–∞–µ—Ç –≤—Å–µ markdown —Ñ–∞–π–ª—ã –∏–∑ english_train/ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ (—Ç–æ–ª—å–∫–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ!)."""
    md_files = glob.glob(os.path.join(DOC_DIR, "*.md"))
    md_files += glob.glob(os.path.join(DOC_DIR, "*.txt"))  # –í–∫–ª—é—á–∞–µ–º .txt —Ñ–∞–π–ª—ã —Ç–æ–∂–µ
    # –ò—Å–∫–ª—é—á–∞–µ–º README.md –∏ –¥—Ä—É–≥–∏–µ —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    md_files = [f for f in md_files if not os.path.basename(f).startswith("README")]
    
    # –í english_train —É–∂–µ —Ç–æ–ª—å–∫–æ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ —Ñ–∞–π–ª—ã, –Ω–æ –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –ø—Ä–æ–≤–µ—Ä—è–µ–º
    return sorted(md_files)

def prepare_text_data():
    """–°–æ–±–∏—Ä–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –∏–∑ markdown —Ñ–∞–π–ª–æ–≤ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞."""
    os.makedirs(DATA_CACHE_DIR, exist_ok=True)
    
    md_files = collect_markdown_files()
    if not md_files:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ markdown —Ñ–∞–π–ª–æ–≤ –≤ {DOC_DIR}/")
        return None
    
    output_file = os.path.join(DATA_CACHE_DIR, "arianna_corpus.txt")
    
    print(f"üìö –°–æ–±–∏—Ä–∞—é —Ç–µ–∫—Å—Ç –∏–∑ {len(md_files)} markdown —Ñ–∞–π–ª–æ–≤...")
    total_size = 0
    
    with open(output_file, "w", encoding="utf-8") as f:
        for md_file in tqdm(md_files):
            try:
                with open(md_file, "r", encoding="utf-8") as inf:
                    content = inf.read().strip()
                    if content:
                        f.write(content + "\n\n")
                        total_size += len(content)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {md_file}: {e}")
    
    file_size_mb = os.path.getsize(output_file) / 1024 / 1024
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω —Ñ–∞–π–ª: {output_file}")
    print(f"üìä –†–∞–∑–º–µ—Ä: {file_size_mb:.2f} MB, —Å–∏–º–≤–æ–ª–æ–≤: {total_size:,}")
    
    return output_file

def train_vocab(vocab_size=4096):
    """
    –û–±—É—á–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–π sentencepiece —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –∫–æ—Ä–ø—É—Å–µ Arianna Method.
    """
    assert vocab_size > 0, "Vocab size must be positive"
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å —Ç–µ–∫—Å—Ç
    corpus_file = prepare_text_data()
    if not corpus_file:
        return
    
    # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    prefix = os.path.join(DATA_CACHE_DIR, f"tok{vocab_size}")
    
    print(f"üß¨ –û–±—É—á–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å vocab_size={vocab_size}...")
    
    spm.SentencePieceTrainer.train(
        input=corpus_file,
        model_prefix=prefix,
        model_type="bpe",
        vocab_size=vocab_size,
        self_test_sample_size=0,
        input_format="text",
        character_coverage=1.0,
        num_threads=os.cpu_count(),
        split_digits=True,
        allow_whitespace_only_pieces=True,
        byte_fallback=True,
        unk_surface=r" \342\201\207 ",
        normalization_rule_name="identity"
    )
    
    print(f"‚úÖ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {prefix}.model")
    print("Done.")

def process_markdown_shard(args, vocab_size):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω markdown —Ñ–∞–π–ª –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –µ–≥–æ."""
    shard_id, md_file = args
    tokenizer_model = get_tokenizer_model_path(vocab_size)
    enc = Tokenizer(tokenizer_model)
    
    try:
        with open(md_file, "r", encoding="utf-8") as f:
            content = f.read().strip()
        
        if not content:
            return
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
        tokens = enc.encode(content, bos=True, eos=False)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ .bin —Ñ–∞–π–ª
        all_tokens = np.array(tokens, dtype=np.uint16)
        
        # –°–æ–∑–¥–∞–µ–º –∏–º—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
        if vocab_size == 0:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Llama 2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            bin_dir = os.path.join(DATA_CACHE_DIR, "arianna_data")
            os.makedirs(bin_dir, exist_ok=True)
            basename = os.path.basename(md_file).replace(".md", ".bin")
            tokenized_filename = os.path.join(bin_dir, basename)
        else:
            # –ö–∞—Å—Ç–æ–º–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
            bin_dir = os.path.join(DATA_CACHE_DIR, f"tok{vocab_size}")
            os.makedirs(bin_dir, exist_ok=True)
            basename = os.path.basename(md_file).replace(".md", ".bin")
            tokenized_filename = os.path.join(bin_dir, basename)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±–∞–π—Ç—ã
        with open(tokenized_filename, "wb") as f:
            f.write(all_tokens.tobytes())
        
        avg_seq_len = all_tokens.size / ((all_tokens == 1).sum()) if (all_tokens == 1).sum() > 0 else all_tokens.size
        print(f"‚úÖ {os.path.basename(tokenized_filename)}, tokens: {len(tokens)}, avg_seq_len: {avg_seq_len:.2f}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {md_file}: {e}")

def pretokenize(vocab_size=0):
    """–ü—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ markdown —Ñ–∞–π–ª—ã."""
    md_files = collect_markdown_files()
    
    if not md_files:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ markdown —Ñ–∞–π–ª–æ–≤ –≤ {DOC_DIR}/")
        return
    
    print(f"üî§ –ü—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é {len(md_files)} —Ñ–∞–π–ª–æ–≤...")
    
    # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è .bin —Ñ–∞–π–ª–æ–≤
    if vocab_size == 0:
        bin_dir = os.path.join(DATA_CACHE_DIR, "arianna_data")
    else:
        bin_dir = os.path.join(DATA_CACHE_DIR, f"tok{vocab_size}")
    os.makedirs(bin_dir, exist_ok=True)
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û (–±–µ–∑ multiprocessing –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å tempdir)
    for idx, md_file in enumerate(md_files):
        process_markdown_shard((idx, md_file), vocab_size)
    
    print("‚úÖ Done.")

class PretokDataset(torch.utils.data.IterableDataset):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ .bin —Ñ–∞–π–ª–æ–≤."""
    
    def __init__(self, split, max_seq_len, vocab_size, vocab_source):
        super().__init__()
        self.split = split
        self.max_seq_len = max_seq_len
        self.vocab_size = vocab_size
        self.vocab_source = vocab_source
    
    def __iter__(self):
        worker_info = torch.utils.data.get_worker_info()
        worker_id = worker_info.id if worker_info else 0
        rank = dist.get_rank() if dist.is_initialized() else 0
        seed = 42 + worker_id + 1337 * rank
        rng = random.Random(seed)
        
        if self.vocab_source == "llama2":
            bin_dir = os.path.join(DATA_CACHE_DIR, "arianna_data")
        elif self.vocab_source == "custom":
            bin_dir = os.path.join(DATA_CACHE_DIR, f"tok{self.vocab_size}")
        else:
            raise ValueError(f"Unknown vocab_source: {self.vocab_source}")
        
        shard_filenames = sorted(glob.glob(os.path.join(bin_dir, "*.bin")))
        
        if not shard_filenames:
            raise ValueError(f"No .bin files found in {bin_dir}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val: 90% train, 10% val
        split_idx = int(len(shard_filenames) * 0.9)
        if self.split == "train":
            shard_filenames = shard_filenames[:split_idx]
        else:
            shard_filenames = shard_filenames[split_idx:]
        
        while True:
            rng.shuffle(shard_filenames)
            for shard in shard_filenames:
                try:
                    m = np.memmap(shard, dtype=np.uint16, mode="r")
                    num_batches = len(m) // self.max_seq_len
                    num_batches -= 1
                    if num_batches <= 0:
                        continue
                    ixs = list(range(num_batches))
                    rng.shuffle(ixs)
                    for ix in ixs:
                        start = ix * self.max_seq_len
                        end = start + self.max_seq_len + 1
                        chunk = torch.from_numpy((m[start:end]).astype(np.int64))
                        x = chunk[:-1]
                        y = chunk[1:]
                        yield x, y
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ {shard}: {e}")
                    continue

def get_tokenizer_model_path(vocab_size):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—É."""
    if vocab_size == 0:
        return None
    else:
        return os.path.join(DATA_CACHE_DIR, f"tok{vocab_size}.model")

class Task:
    """Task –∫–ª–∞—Å—Å –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ train.py."""
    
    @staticmethod
    def iter_batches(batch_size, device, num_workers=0, **dataset_kwargs):
        ds = PretokDataset(**dataset_kwargs)
        dl = torch.utils.data.DataLoader(
            ds, batch_size=batch_size, pin_memory=True, num_workers=num_workers
        )
        for x, y in dl:
            x = x.to(device, non_blocking=True)
            y = y.to(device, non_blocking=True)
            yield x, y

if __name__ == "__main__":
    """
    –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    
    # –° –∫–∞—Å—Ç–æ–º–Ω—ã–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –∫–æ—Ä–ø—É—Å–∞):
    python arianna_data.py train_vocab --vocab_size=4096
    python arianna_data.py pretokenize --vocab_size=4096
    
    # –° Llama 2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º:
    python arianna_data.py pretokenize
    """
    parser = argparse.ArgumentParser(description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö Arianna Method –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
    parser.add_argument("stage", type=str, choices=["prepare", "train_vocab", "pretokenize"],
                       help="–≠—Ç–∞–ø –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
    parser.add_argument("--vocab_size", type=int, default=0,
                       help="–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è. 0 = –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Llama 2 —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä")
    args = parser.parse_args()
    
    if args.stage == "prepare":
        prepare_text_data()
    elif args.stage == "train_vocab":
        train_vocab(vocab_size=args.vocab_size)
    elif args.stage == "pretokenize":
        pretokenize(vocab_size=args.vocab_size)
    else:
        raise ValueError(f"Unknown stage: {args.stage}")

