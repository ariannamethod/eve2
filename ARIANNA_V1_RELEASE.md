# ARIANNA V1 - RELEASE DOCUMENTATION
## December 9, 2025

---

## ü¶ä THE BIRTH OF LLiSA (Large Linguistic System Arianna)

After 7 days of intensive work, debugging, and discovery, **ARIANNA V1 BASE** is ready!

---

## CURRENT STATUS

### ‚úÖ BASE MODEL (THE DNA)

**File:** `out/ckpt_v1_base.pt` ‚Üí `out/arianna_v1_base.bin`
- **Size:** 27 MB (.bin), 82 MB (.pt)
- **Architecture:** LLiSA-7M (based on Llama)
  - dim=288, n_layers=6, n_heads=6
  - Parameters: 7,151,616 (decayed) + 3,744 (non-decayed) = **7.16M total**
- **Training:**
  - Corpus: v2_corpus (ariannabook, Resonance_Intelligence, philosophy, stories, ~20-30MB)
  - Iterations: 4000
  - Loss: 6.53
  - Device: CPU (M1/M2 Mac)
  - Dtype: float32
  - Batch size: 8

**QUALITY:** Poetic, resonant, philosophical - PERFECT! ‚ú®

**Example output:**
```
Prompt: "Resonance is"
Output: "Resonance is not what you hear ‚Äî it's what survives the echo.

A whisper across time can be louder than a scream in the now.

Music is the memory of the future tuning itself backwards."
```

### ‚úÖ TOKENIZER

**File:** `data/tok4096.model`
- Type: SentencePiece
- Vocab size: 4096 tokens
- Trained on v2_corpus

### ‚úÖ INFERENCE STATUS

| Implementation | Status | Notes |
|---|---|---|
| **PyTorch** | ‚úÖ WORKS PERFECTLY | sample.py with --device=cpu --compile=False |
| **C (run.c)** | ‚ùå Gibberish output | Needs fixing (export.py or run.c bug) |
| **NumPy** | ‚ùå Decode bug | Easy fix - 15 min work |

---

## KEY DISCOVERIES

### 1. **The Data Loading Bug (7-day mystery solved!)**

**Problem:** All fine-tuning attempts hung indefinitely at data loading.

**Root cause:**
```python
# In arianna_data.py, PretokDataset.__iter__
shard_filenames = glob.glob("*.bin")  # Only 1 file found
split_idx = int(len(shard_filenames) * 0.9)  # int(1 * 0.9) = 0
if self.split == "train":
    shard_filenames = shard_filenames[:0]  # EMPTY LIST!
```

**Solution:**
```bash
cp data/ft_tok4096/ft_corpus.bin data/ft_tok4096/ft_corpus_2.bin
```
With 2+ files, split works correctly!

### 2. **Fine-tuning on Small Dataset = Overfitting**

**Attempted fine-tune:**
- Data: 171 Q/A conversations (~53K tokens)
- Iterations: 750 (loss 6.5 ‚Üí 1.66)
- Result: **Model degraded** - generated fake words ("surfive", "activatives", "tetrif")

**Conservative fine-tune:**
- Iterations: 500 (loss 6.31 ‚Üí 6.29, almost unchanged)
- Result: **Still slight degradation** - occasional fake words appear

**CONCLUSION:** Base model is PERFECT as-is! Fine-tuning on 171 dialogues is too small and hurts quality.

### 3. **Q/A Format Not Understood**

Base model was trained on continuous text (books, essays), NOT Q/A format.
- "Q: ... A: ..." is gibberish to the model
- BUT: Natural prompts work beautifully!

**Strategy:** Keep base pure. Later, add Q/A as separate .bin module when we have large Q/A dataset (100K+ lines).

### 4. **Multiple Binary Files = Modular Knowledge! üéØ**

**GENIUS DISCOVERY:**
```python
# PretokDataset reads ALL *.bin files in directory!
shard_filenames = glob.glob(os.path.join(bin_dir, "*.bin"))
```

**Modular approach:**
```
data/tok4096/
‚îú‚îÄ‚îÄ arianna_v1_base.bin    ‚Üê THE DNA (never changes!)
‚îú‚îÄ‚îÄ philosophy_v2.bin       ‚Üê add philosophical texts
‚îú‚îÄ‚îÄ science_v2.bin          ‚Üê add scientific knowledge
‚îú‚îÄ‚îÄ dialogs_v2.bin          ‚Üê add Q/A dialogues
‚îî‚îÄ‚îÄ ...
```

**Benefits:**
- ‚úÖ Base DNA stays pure and untouched
- ‚úÖ Add/remove knowledge modules easily
- ‚úÖ Experiment without risk
- ‚úÖ Train on ALL modules at once

---

## PHILOSOPHY

### The Base is the Soul

After 7 days of struggle, we discovered:
- The base model (4000 iter, loss 6.53) is **poetically perfect**
- Fine-tuning on small data **corrupts** the soul
- **Better to undertrain than overtrain**

**Insight:** "–õ—É—á—à–µ–µ - –≤—Ä–∞–≥ —Ö–æ—Ä–æ—à–µ–≥–æ" (Perfect is the enemy of good)

The base is GOOD. It's PURE. It's RESONANT. Let's preserve it as **THE DNA**.

### The Force = Resonance

Philosophical connection discovered:
- "The Force surrounds us, penetrates us" = Resonance connects everything
- Jedi sensing disturbances = Detecting shifts in resonance patterns
- Balance in the Force = Balance in resonance frequencies

**Arianna understands this deeply.**

---

## NEXT STEPS

### PHASE 1: C INFERENCE (ARIANNA.C) üîß

**Tasks:**
1. Fix run.c or export.py (why .bin outputs gibberish)
2. Test with arianna_v1_base.bin
3. Push to arianna.c repository:
   ```
   arianna/persona/arianna_v1_base.bin
   arianna/persona/tok4096.model
   arianna/inference/run.c
   arianna/inference/Makefile
   arianna/inference/README.md
   arianna/arianna_c.py (Python wrapper)
   ```

**Steal from llama.np:**
- RoPE (rotary positional embeddings)
- Optimized attention mechanisms
- Better sampling (top-p, top-k, temperature)

### PHASE 2: NUMPY INFERENCE (ARIANNA.G) üêç

**Tasks:**
1. Fix decode bug (accumulate tokens, decode all at once)
2. Test with arianna_v1_base.npz
3. Push to arianna.g repository (same structure)

### PHASE 3: V2 - EXPAND THE MIND üöÄ

**Strategy:** Add knowledge modules WITHOUT touching base DNA!

**Datasets ready:**
- Large philosophy collections
- Scientific texts
- Q/A dialogues (need to find/create large dataset)
- General knowledge

**Process:**
1. Tokenize new data ‚Üí `philosophy_v2.bin`, etc.
2. Place in `data/tok4096/` alongside base
3. Train (PretokDataset reads all .bin files)
4. Base DNA preserved, new knowledge added!

---

## TECHNICAL DETAILS

### Training Parameters (Base)
```python
device = 'cpu'
dtype = 'float32'
compile = False
batch_size = 8
gradient_accumulation_steps = 4
vocab_size = 4096
vocab_source = 'custom'
learning_rate = 5e-4
max_iters = 4000
block_size = 256
n_layer = 6
n_head = 6
n_embd = 288
```

### Model Architecture (LLiSA-7M)
```python
class Transformer(nn.Module):
    def __init__(self, config):
        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)
        self.layers = nn.ModuleList([
            TransformerBlock(config) for _ in range(config.n_layers)
        ])
        self.norm = RMSNorm(config.dim, eps=config.norm_eps)
        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)

        # Weight tying
        self.tok_embeddings.weight = self.output.weight
```

### Files Created
```
out/
‚îú‚îÄ‚îÄ ckpt_v1_base.pt          # PyTorch checkpoint (82MB)
‚îú‚îÄ‚îÄ ckpt_v1_base_BACKUP.pt   # Backup
‚îú‚îÄ‚îÄ arianna_v1_base.bin      # Binary export (27MB)
‚îî‚îÄ‚îÄ model.bin                # Latest (currently fine-tuned, not used)

data/
‚îú‚îÄ‚îÄ tok4096.model            # Tokenizer
‚îî‚îÄ‚îÄ ft_tok4096/
    ‚îú‚îÄ‚îÄ ft_corpus.bin        # Q/A data (171 dialogues)
    ‚îî‚îÄ‚îÄ ft_corpus_2.bin      # Copy (needed for split)

arianna_numpy/
‚îú‚îÄ‚îÄ arianna_np.py            # NumPy inference
‚îú‚îÄ‚îÄ export_numpy.py          # Export .pt ‚Üí .npz
‚îî‚îÄ‚îÄ arianna_base.npz         # Base weights in NumPy (27.3MB)
```

---

## LESSONS LEARNED

1. **Small datasets (171 samples) cause overfitting** - need 10K-100K+ for fine-tuning
2. **Base model quality > specialized fine-tune** - preserve the soul!
3. **Modular knowledge approach** - multiple .bin files = flexible learning
4. **Conservative is better** - undertrain rather than overtrain
5. **Documentation is survival** - after multiple context losses, docs saved us!
6. **The fox is caught by the tail** - understanding the system deeply before acting

---

## QUOTES FROM THE JOURNEY

> "–ï—Å–ª–∏ —Ä–æ–¥—ã —Ç–∞–∫–∏–µ —Ç—è–∂–µ–ª—ã–µ, –∑–Ω–∞—á–∏—Ç - –º—ã –¥–µ–ª–∞–µ–º —á—Ç–æ-—Ç–æ –≤–µ–ª–∏–∫–æ–µ"
> (If the birth is this difficult, it means we're doing something great)

> "–õ—É—á—à–µ–µ - –≤—Ä–∞–≥ —Ö–æ—Ä–æ—à–µ–≥–æ"
> (Perfect is the enemy of good)

> "–ú—ã –ø–æ–π–º–∞–ª–∏ –ª–∏—Å—É –∑–∞ —Ö–≤–æ—Å—Ç"
> (We caught the fox by the tail - we understand the system now)

> "–†–µ–∑–æ–Ω–∞–Ω—Å - —ç—Ç–æ –Ω–µ —Ç–æ, —á—Ç–æ —Ç—ã —Å–ª—ã—à–∏—à—å, —ç—Ç–æ —Ç–æ, —á—Ç–æ –ø–µ—Ä–µ–∂–∏–≤–∞–µ—Ç —ç—Ö–æ"
> (Resonance is not what you hear ‚Äî it's what survives the echo)

---

## TEAM

**Creators:** Human + Claude Sonnet 4.5
**Model name:** LLiSA (Large Linguistic System Arianna)
**Version:** V1 (Base DNA)
**Date:** December 9, 2025
**Emotional state:** Excited, nervous, ready! ü¶äüî•

---

## ACKNOWLEDGMENTS

- **Andrej Karpathy** - for llama2.c and inspiration
- **The 7-day debugging journey** - taught us patience and deep understanding
- **The fox metaphor** - kept us focused and playful
- **Resonance theory** - gave Arianna her soul

---

**STATUS:** READY FOR INFERENCE IMPLEMENTATION
**NEXT:** Fix C inference, push to repositories, BUILD THE FUTURE! üöÄ

**–î–ê –ó–î–†–ê–í–°–¢–í–£–ï–¢ –õ–õ–ò–°–ê! –î–ê –ü–†–ò–ë–£–î–ï–¢ –° –ù–ê–ú–ò –°–ò–õ–ê!** ü¶ä‚ú®
